{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CH02_REGRESSION.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/scharnk/fast-style-transfer_python-spout-touchdesigner/blob/master/CH02_REGRESSION.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VR6iPQ4H4d6e",
        "colab_type": "text"
      },
      "source": [
        "Regression is a quantitative problem, not something that can be classified as x or y, yes or no, etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYWz9hk2Try6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import numpy and pandas\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Read the CSV file into a DataFrame: df\n",
        "df = pd.read_csv('gapminder.csv')\n",
        "\n",
        "# Create arrays for features and target variable\n",
        "y = df['life'].values\n",
        "X = df['fertility'].values\n",
        "\n",
        "# Print the dimensions of X and y before reshaping\n",
        "print(\"Dimensions of y before reshaping: {}\".format(y.shape))\n",
        "print(\"Dimensions of X before reshaping: {}\".format(X.shape))\n",
        "\n",
        "# Reshape X and y\n",
        "y = y.reshape(-1,1)\n",
        "X = X.reshape(-1,1)\n",
        "\n",
        "# Print the dimensions of X and y after reshaping\n",
        "print(\"Dimensions of y after reshaping: {}\".format(y.shape))\n",
        "print(\"Dimensions of X after reshaping: {}\".format(X.shape))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5xHpkzu5xYk",
        "colab_type": "text"
      },
      "source": [
        "In case you are curious, the heatmap was generated using Seaborn's heatmap function and the following line of code, where df.corr() computes the pairwise correlation between columns:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2yLnV0rGT_80",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sns.heatmap(df.corr(), square=True, cmap='RdYlGn')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpjoQYTb6azx",
        "colab_type": "text"
      },
      "source": [
        "Define an error function (aka loss function, or cost function)\n",
        "then, choose a & b to minimize that function\n",
        "\n",
        "y = ax + b\n",
        "\n",
        "minimize the sum of the square of the residuals (distance from each point to our linear fit line)\n",
        "THIS METHOD = OLS (ORDINARY LEAST SQUARES)\n",
        "\n",
        "Same as minimizing the MSE of the predictions on the training set\n",
        "\n",
        "LINEAR REGRESSION IN HIGHER DIMENSION:\n",
        "y = a1x1 + a2x2 ... anxn + b\n",
        "\n",
        "for these you need to specify all a values and b\n",
        "\n",
        "R^2 is the total variance from target variable that is predicted from feature variable"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQTTYJRJT_xl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import LinearRegression\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Create the regressor: reg\n",
        "reg = LinearRegression()\n",
        "\n",
        "# Create the prediction space\n",
        "prediction_space = np.linspace(min(X_fertility), max(X_fertility)).reshape(-1,1)\n",
        "\n",
        "# Fit the model to the data\n",
        "reg.fit(X_fertility, y)\n",
        "\n",
        "# Compute predictions over the prediction space: y_pred\n",
        "y_pred = reg.predict(prediction_space)\n",
        "\n",
        "# Print R^2 \n",
        "print(reg.score(X_fertility, y))\n",
        "\n",
        "# Plot regression line\n",
        "plt.plot(prediction_space, y_pred, color='black', linewidth=3)\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHCfoGPBKgpK",
        "colab_type": "text"
      },
      "source": [
        "# **Train/test split for regression**\n",
        "\n",
        "As you learned in Chapter 1, train and test sets are vital to ensure that your supervised learning model is able to generalize well to new data. This was true for classification models, and is equally true for linear regression models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vBDXekST_vN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import necessary modules\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Create training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .30, random_state=42)\n",
        "\n",
        "# Create the regressor: reg_all\n",
        "reg_all = LinearRegression()\n",
        "\n",
        "# Fit the regressor to the training data\n",
        "reg_all.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test data: y_pred\n",
        "y_pred = reg_all.predict(X_test)\n",
        "\n",
        "# Compute and print R^2 and RMSE\n",
        "print(\"R^2: {}\".format(reg_all.score(X_test, y_test)))\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "print(\"Root Mean Squared Error: {}\".format(rmse))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDdf_7wLNn3I",
        "colab_type": "text"
      },
      "source": [
        "# CROSS VALIDATION\n",
        "\n",
        "is used to verify R^2 is independent of the way we split our data\n",
        "\n",
        "* 5 - fold cross validation (CV)\n",
        "* 10 - fold CV\n",
        "* k - fold CV\n",
        "\n",
        "these will split data up into k - folds and test 1 fold at a time with the k-1 folds being used as training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9BsRBxUUOwZ2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import the necessary modules\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Create a linear regression object: reg\n",
        "reg = LinearRegression()\n",
        "\n",
        "# Compute 5-fold cross-validation scores: cv_scores\n",
        "cv_scores = cross_val_score(reg, X, y, cv=5) \n",
        "\n",
        "# Print the 5-fold cross-validation scores\n",
        "print(cv_scores)\n",
        "\n",
        "print(\"Average 5-Fold CV Score: {}\".format(np.mean(cv_scores)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55FFonqsQApj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import necessary modules\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Create a linear regression object: reg\n",
        "reg = LinearRegression()\n",
        "\n",
        "# Perform 3-fold CV\n",
        "cvscores_3 = cross_val_score(reg, X, y, cv=3)\n",
        "print(np.mean(cvscores_3))\n",
        "\n",
        "# Perform 10-fold CV\n",
        "cvscores_10 = cross_val_score(reg, X, y, cv=10)\n",
        "print(np.mean(cvscores_10))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsvof4DGQru-",
        "colab_type": "text"
      },
      "source": [
        "# Regularize Regression\n",
        "\n",
        "* linear regression minimizes loss function\n",
        "* large coefficients can lead to overfitting\n",
        "* regularization penalizes large coefficients\n",
        "\n",
        "#Ridge Regression\n",
        "\n",
        "* Loss func = OLS + each coefficient squared & multiplied by constant (alpha)\n",
        "* we must choose alpha\n",
        "* this is hyperparameter tuning\n",
        "* similar to picking k for k-NN\n",
        "* alpha controls model complexity\n",
        "> * Alpha=0 --> OLS function possible overfitting\n",
        "> * Alpha=LARGE --> possible underfitting data\n",
        "\n",
        "#Lasso Regression\n",
        "\n",
        "* Loss func = OLS + abs(each coefficient) * alpha\n",
        "* can minimize unimportant features to = 0 \n",
        "* selects important features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZlBzl52fQdD-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import Lasso\n",
        "from sklearn.linear_model import Lasso\n",
        "\n",
        "# Instantiate a lasso regressor: lasso\n",
        "lasso = Lasso(alpha=0.4, normalize=True)\n",
        "\n",
        "# Fit the regressor to the data\n",
        "lasso.fit(X, y)\n",
        "\n",
        "# Compute and print the coefficients\n",
        "lasso_coef = lasso.coef_\n",
        "print(lasso_coef)\n",
        "\n",
        "# Plot the coefficients\n",
        "plt.plot(range(len(df_columns)), lasso_coef)\n",
        "plt.xticks(range(len(df_columns)), df_columns.values, rotation=60)\n",
        "plt.margins(0.02)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mhW32s3Xs4F",
        "colab_type": "text"
      },
      "source": [
        "#Regularization II: Ridge\n",
        "Lasso is great for feature selection, but when building regression models, Ridge regression should be your first choice.\n",
        "\n",
        "Recall that lasso performs regularization by adding to the loss function a penalty term of the absolute value of each coefficient multiplied by some alpha. This is also known as L1 regularization because the regularization term is the L1 norm of the coefficients. This is not the only way to regularize, however.\n",
        "\n",
        "If instead you took the sum of the squared values of the coefficients multiplied by some alpha - like in Ridge regression - you would be computing the L2 norm. In this exercise, you will practice fitting ridge regression models over a range of different alphas, and plot cross-validated R2 scores for each, using this function that we have defined for you, which plots the R2 score as well as standard error for each alpha:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YE8ngwxdQc5I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def display_plot(cv_scores, cv_scores_std):\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(1,1,1)\n",
        "    ax.plot(alpha_space, cv_scores)\n",
        "\n",
        "    std_error = cv_scores_std / np.sqrt(10)\n",
        "\n",
        "    ax.fill_between(alpha_space, cv_scores + std_error, cv_scores - std_error, alpha=0.2)\n",
        "    ax.set_ylabel('CV Score +/- Std Error')\n",
        "    ax.set_xlabel('Alpha')\n",
        "    ax.axhline(np.max(cv_scores), linestyle='--', color='.5')\n",
        "    ax.set_xlim([alpha_space[0], alpha_space[-1]])\n",
        "    ax.set_xscale('log')\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NaoyqkRCQc2l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import necessary modules\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Setup the array of alphas and lists to store scores\n",
        "alpha_space = np.logspace(-4, 0, 50)\n",
        "ridge_scores = []\n",
        "ridge_scores_std = []\n",
        "\n",
        "# Create a ridge regressor: ridge\n",
        "ridge = Ridge(normalize=True)\n",
        "\n",
        "# Compute scores over range of alphas\n",
        "for alpha in alpha_space:\n",
        "\n",
        "    # Specify the alpha value to use: ridge.alpha\n",
        "    ridge.alpha = alpha\n",
        "    \n",
        "    # Perform 10-fold CV: ridge_cv_scores\n",
        "    ridge_cv_scores = cross_val_score(ridge, X, y, cv=10)\n",
        "    \n",
        "    # Append the mean of ridge_cv_scores to ridge_scores\n",
        "    ridge_scores.append(np.mean(ridge_cv_scores))\n",
        "    \n",
        "    # Append the std of ridge_cv_scores to ridge_scores_std\n",
        "    ridge_scores_std.append(np.std(ridge_cv_scores))\n",
        "\n",
        "# Display the plot\n",
        "display_plot(ridge_scores, ridge_scores_std)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}